from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id, udf
import requests

"""
#==================================================================#
### build_mailware_jsons.py - Read Mailware Data into JSON files ###
#------------------------------------------------------------------#

This program looks for data sources that match thsoe provied in the University
of Georgia computer science course CSCI 8360 and stores the data in foramtted
json files for later use. In our course, we were supplied with seven files
(all files listed reside in the data/ folder and gs://uga-dsp/project1/files/):
    
    X_small_train.txt, y_small_train.txt : The filenames and malware categories
                                           of a small training set
    X_small_test.txt, y_small_test.txt : The filenames and malware categories
                                         of a small test set
    X_train.txt, y_train.txt : The filenames and malware categories of a large
                               training set
    X_test.txt : The files names of the large test set used to evaluate 
                 software performance
                
The filesnames are prefixes that could define target files by appending
".bytes" for byte files or ".asm" for asm files. In general, exexute the
function build_all_json with a target directory to build all json files
possible of a specific type. For example, executing
    
    build_all_json("gs://project-name/data","bytes")

would generate the following json files in the directory gs://project-name/data

    train_small_bytes.json
    test_small_bytes.json
    train_full_bytes.json
    test_full_bytes.json

You may generate any of the four files specifically by calling the function
build_json specifying the output file, data group, and data type. For example:
    
    build_json("gs://project-name/data/train_small_asm","small_train","asm")
    
will generate the json file gs://project-name/data/train_small_asm with asm
data from the small training data set

"""

spark_active = None
full_mode = False
allowed_file_types = ('bytes','asm')
allowed_target_groups = ('small_train','small_test','full_train','full_test')
bytes_base_url = 'gs://uga-dsp/project1/data/bytes/'
asm_base_url = 'gs://uga-dsp/project1/data/asm/'
proj_file_dir = 'gs://uga-dsp/project1/files/'
small_x_train = 'X_small_train.txt'
small_y_train = 'X_small_train.txt'
small_x_test = 'X_small_test.txt'
small_y_test = 'y_small_test.txt'
full_x_train = 'X_train.txt'
full_y_train = 'y_train.txt'
full_x_test = 'X_test.txt'
repartition_count = 40


# Function to add text to a string
def add_bytes_text(s):
    return (s+'.bytes')

# Function to read byte file for each hash input and return the entire file 
# as a single string
def text_bytes_get(fileName): 
	resp = requests.get(bytes_base_url + fileName).text
	resp_filtered = ' '.join([w for w in resp.split() if len(w)<3])
	return(resp_filtered)

# Function to add text to a string
def add_asm_text(s):
    return (s+'.asm')

# Function to read byte file for each hash input and return the entire file as 
# a single string
def text_asm_get(fileName):   
	resp = requests.get(asm_base_url + fileName).text
	lines = resp.splitlines()
	firsts = [line.split("\\s+")[0] for line in lines]
	first = [word if ":" not in word else word.split(':')[0] for word in firsts]
	varList = " ".join(first)
	return(varList)

# Decarling the functions as udf
add_bytes_text_udf = udf(add_bytes_text)
text_bytes_get_udf = udf(text_bytes_get)
add_asm_text_udf = udf(add_asm_text)
text_asm_get_udf = udf(text_asm_get)

def build_json(output_directory, target_group, file_type = "bytes"):
    if( file_type not in allowed_file_types):
        raise ValueError("Only file_types 'bytes' and 'asm' are recognized")
    if( target_group not in allowed_target_groups ):
        raise ValueError("Only target groups allowed are small_train, " + \
                         "small_test, full_train, and full_test - you passed" +\
                         " '" + target_group + "'" )
    if( not full_mode ):
        spark_active = SparkSession.builder.master("yarn").appName(
                        "DSP P1- Train & Test set creation").getOrCreate()
    
    target_x_file = None
    target_y_file = None
    if(target_group == "small_train"):
        target_x_file = proj_file_dir + small_x_train
        target_y_file = proj_file_dir + small_y_train
    if(target_group == "small_test"):
        target_x_file = proj_file_dir + small_x_test
        target_y_file = proj_file_dir + small_y_test
    if(target_group == "full_train"):
        target_x_file = proj_file_dir + full_x_train
        target_y_file = proj_file_dir + full_y_train
    if(target_group == "full_test"):
        target_x_file = proj_file_dir + full_x_test
    
    if(target_x_file == None):
        print("Output: '" + output_directory + "', target_group: '" + target_group + "', file_type: '" + file_type + "'")
        raise ValueError("target_x_file was not initialized")
    
    # Read filenames
    print("target_x_file: " + target_x_file)
    files_list = spark_active.read.option("header", "false") \
        .csv(target_x_file) \
        .withColumn("row_id", monotonically_increasing_id())
    if(file_type == "bytes"):
        files_list = files_list.withColumn('filename', 
                                           add_bytes_text_udf('_c0')).drop('_c0')
    if(file_type == "asm"):
        files_list = files_list.withColumn('filename', 
                                           add_asm_text_udf('_c0')).drop('_c0')
    
    # Compensate for categories if category file is defined
    df_1 = files_list
    if( target_y_file != None ):
        print("target_y_file: " + target_y_file)
        # Read categories
        cat_list = spark_active.read.option("header", "false") \
            .csv(target_y_file) \
            .withColumn("row_id", monotonically_increasing_id()) \
            .withColumnRenamed('_c0','category')
        
        df_1 = files_list.join(cat_list,'row_id',how = 'left') \
            .repartition(repartition_count)

    # Read the text for target bytes or asm files
    final_df = df_1
    if( file_type == "bytes" ):
        final_df = df_1.withColumn('bytes_text',text_bytes_get_udf("filename")) \
            .repartition(repartition_count)
    if( file_type == "asm" ):
        final_df = df_1.withColumn('asm_text',text_asm_get_udf("filename")) \
            .repartition(repartition_count)
    
    if( not output_directory.endswith("/") ):
        output_directory = output_directory + "/"
    print("\nOUTPUT: '" + output_directory + target_group + file_type + "'\n")
    
    final_df.write.json(output_directory + target_group + file_type,mode = 'overwrite')
    
    if( not full_mode ):
        spark_active.stop()
    
def build_all_json(output_directory, file_type):
    if( file_type not in allowed_file_types):
        raise ValueError("Only file_types 'bytes' and 'asm' are recognized")
    
    spark_active = SparkSession.builder.master("yarn").appName(
                    "DSP P1- Train & Test set creation").getOrCreate()
    full_mode = True
    
    if( not output_directory.endswith("/") ):
        output_directory = output_directory + "/"
    
    for group in allowed_target_groups:
        build_json(output_directory + group + file_type, group, file_type)
    
    full_mode = False
    spark_active.close()

