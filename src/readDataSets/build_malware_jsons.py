#==================================================================#
### build_mailware_jsons.py - Read Mailware Data into JSON files ###
#------------------------------------------------------------------#
#
#This program looks for data sources that match thsoe provied in the University
#of Georgia computer science course CSCI 8360 and stores the data in foramtted
#json files for later use. In our course, we were supplied with seven files
#(all files listed reside in the data/ folder and gs://uga-dsp/project1/files/):
#
#    X_small_train.txt, y_small_train.txt : The filenames and malware categories
#                                           of a small training set
#    X_small_test.txt, y_small_test.txt : The filenames and malware categories
#                                         of a small test set
#    X_train.txt, y_train.txt : The filenames and malware categories of a large
#                               training set
#    X_test.txt : The files names of the large test set used to evaluate
#                 software performance
#
#The filesnames are prefixes that could define target files by appending
#".bytes" for byte files or ".asm" for asm files. In general, exexute the
#function build_all_json with a target directory to build all json files
#possible of a specific type. For example, executing
#
#    build_all_json("gs://project-name/data","bytes")
#
#would generate the following json files in the directory gs://project-name/data
#
#    train_small_bytes.json
#    test_small_bytes.json
#    train_full_bytes.json
#    test_full_bytes.json
#
#You may generate any of the four files specifically by calling the function
#build_json specifying the output file, data group, and data type. For example:
#
#    build_json("gs://project-name/data/train_small_asm","small_train","asm")
#
#will generate the json file gs://project-name/data/train_small_asm with asm
#data from the small training data set
#


from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id, udf, lower
from pyspark.sql.functions import concat, col, lit
import requests


allowed_file_types = ('bytes','asm')
allowed_target_groups = ('small_train','small_test','full_train','full_test')
spark_active = None
full_mode = False


def add_bytes_text(s):
    """
    Adds the .bytes extension to the string/filename
    :param s: String, i.e, filenames
    :return: String with .bytes extension
    """
    return (s+'.bytes')


def text_bytes_get(fileName):
    """
        Returns a string formed by joining all the byte files, mapped with their corresponding hashes
        :param fileName: Corresponds to the hash of the bytefile
        :return: String
    """
    resp = requests.get(bytes_base_url + fileName).text
	resp_filtered = ' '.join([w for w in resp.split() if len(w)<3])
	return(resp_filtered)


def add_asm_text(s):
    """
    Adds the .asm extension to the string/filename
    :param s: String, i.e, filename
    :return: String with .asm extension
    """
    return (s+'.asm')


def text_asm_get(fileName):
    """
    Reads asm files for each hash input and returns entire file as single string
    :param fileName: Corresponds to the hash of the asm file
    :return: String
    """
    resp = requests.get(asm_base_url + fileName).text
	lines = resp.splitlines()
	firsts = [line.split("\\s+")[0] for line in lines]
	first = [word if ":" not in word else word.split(':')[0] for word in firsts]
	varList = " ".join(first)
	return(varList)


# Decarling the functions as udf
add_bytes_text_udf = udf(add_bytes_text)
text_bytes_get_udf = udf(text_bytes_get)
add_asm_text_udf = udf(add_asm_text)
text_asm_get_udf = udf(text_asm_get)

def build_json(output_directory, target_group, file_type = "bytes",
               bytes_base_url = 'https://storage.googleapis.com/uga-dsp/project1/data/bytes/',
               asm_base_url = 'https://storage.googleapis.com/uga-dsp/project1/data/asm/',
               proj_file_dir = 'gs://uga-dsp/project1/files/',
               small_x_train = 'X_small_train.txt',
               small_y_train = 'X_small_train.txt',
               small_x_test = 'X_small_test.txt',
               small_y_test = 'y_small_test.txt',
               full_x_train = 'X_train.txt',
               full_y_train = 'y_train.txt',
               full_x_test = 'X_test.txt',
               repartition_count = 40):
    """
    Builds the JSON files for training and testing data in bytes files.
    :param output_directory: Directory path to save the outputs
    :param target_group: Small/ Full
    :param file_type: Either JSON or Bytes
    :param bytes_base_url: Path or URL of directory containing bytes files
    :param asm_base_url: Path or URL of directory containing ASM files
    :param proj_file_dir: Path or URL of directory containing set of files
    :param small_x_train: Name of small training file containing target filenames
    :param small_y_train: Name of small training file containing labels
    :param small_x_test: Name of small testing file containing target filenames
    :param small_y_test: Name of small testing file containing labels
    :param full_x_train: Name of full training file containing target filenames
    :param full_y_train: Name of full training file containing labels
    :param full_x_test: Name of full testing file containing target filenames
    :param repartition_count: Repartition count value
    :return: Writes bytes data into JSON files
    """
    if( file_type not in allowed_file_types):
        raise ValueError("Only file_types 'bytes' and 'asm' are recognized")
    if( target_group not in allowed_target_groups ):
        raise ValueError("Only target groups allowed are small_train, " + \
                         "small_test, full_train, and full_test - you passed" +\
                         " '" + target_group + "'" )
    if( not full_mode ):
        spark_active = SparkSession.builder.master("yarn").appName(
                        "DSP P1- Train & Test set creation").getOrCreate()
    
    target_x_file = None
    target_y_file = None
    if(target_group == "small_train"):
        target_x_file = proj_file_dir + small_x_train
        target_y_file = proj_file_dir + small_y_train
    if(target_group == "small_test"):
        target_x_file = proj_file_dir + small_x_test
        target_y_file = proj_file_dir + small_y_test
    if(target_group == "full_train"):
        target_x_file = proj_file_dir + full_x_train
        target_y_file = proj_file_dir + full_y_train
    if(target_group == "full_test"):
        target_x_file = proj_file_dir + full_x_test
    
    if(target_x_file == None):
        print("Output: '" + output_directory + "', target_group: '" + target_group + "', file_type: '" + file_type + "'")
        raise ValueError("target_x_file was not initialized")
    
    # Read filenames
    files_list = spark_active.read.option("header", "false") \
        .csv(target_x_file) \
        .withColumn("row_id", monotonically_increasing_id())
    if(file_type == "bytes"):
        files_list = files_list.withColumn('filename', 
                                           add_bytes_text_udf('_c0')).drop('_c0')
    if(file_type == "asm"):
        files_list = files_list.withColumn('filename', 
                                           add_asm_text_udf('_c0')).drop('_c0')
    
    # Compensate for categories if category file is defined
    df_1 = files_list
    if( target_y_file != None ):
        # Read categories
        cat_list = spark_active.read.option("header", "false") \
            .csv(target_y_file) \
            .withColumn("row_id", monotonically_increasing_id()) \
            .withColumnRenamed('_c0','category')
        
        df_1 = files_list.join(cat_list,'row_id',how = 'left') \
            .repartition(repartition_count)

    # Read the text for target bytes or asm files
    final_df = df_1
    if( file_type == "bytes" ):
        final_df = df_1.withColumn('text',text_bytes_get_udf("filename")) \
            .repartition(repartition_count)
    if( file_type == "asm" ):
        final_df = df_1.withColumn('text',text_asm_get_udf("filename")) \
            .repartition(repartition_count)
    
    if( not output_directory.endswith("/") ):
        output_directory = output_directory + "/"
    
    final_df.write.json(output_directory + target_group + "_" + file_type,mode = 'overwrite')
    
    if( not full_mode ):
        spark_active.stop()
    
def build_all_json(output_directory, file_type,
               bytes_base_url = 'https://storage.googleapis.com/uga-dsp/project1/data/bytes/',
               asm_base_url = 'https://storage.googleapis.com/uga-dsp/project1/data/asm/',
               proj_file_dir = 'gs://uga-dsp/project1/files/',
               small_x_train = 'X_small_train.txt',
               small_y_train = 'X_small_train.txt',
               small_x_test = 'X_small_test.txt',
               small_y_test = 'y_small_test.txt',
               full_x_train = 'X_train.txt',
               full_y_train = 'y_train.txt',
               full_x_test = 'X_test.txt',
               repartition_count = 40):
    """
    Builds JSON files for the training and testing data of ASM files.
    :param output_directory: Directory path to save the outputs
    :param target_group: Small/ Full
    :param file_type: Either JSON or Bytes
    :param bytes_base_url: Path or URL of directory containing bytes files
    :param asm_base_url: Path or URL of directory containing ASM files
    :param proj_file_dir: Path or URL of directory containing set of files
    :param small_x_train: Name of small training file containing target filenames
    :param small_y_train: Name of small training file containing labels
    :param small_x_test: Name of small testing file containing target filenames
    :param small_y_test: Name of small testing file containing labels
    :param full_x_train: Name of full training file containing target filenames
    :param full_y_train: Name of full training file containing labels
    :param full_x_test: Name of full testing file containing target filenames
    :param repartition_count: Repartition count value
    :return: Writes ASM data into JSON files
    """
    if( file_type not in allowed_file_types):
        raise ValueError("Only file_types 'bytes' and 'asm' are recognized")
    
    spark_active = SparkSession.builder.master("yarn").appName(
                    "DSP P1- Train & Test set creation").getOrCreate()
    full_mode = True
    
    if( not output_directory.endswith("/") ):
        output_directory = output_directory + "/"
    
    for group in allowed_target_groups:
        build_json(output_directory + group + file_type, group, file_type,
                   bytes_base_url,
                   asm_base_url,
                   proj_file_dir,
                   small_x_train,
                   small_y_train,
                   small_x_test,
                   small_y_test,
                   full_x_train,
                   full_y_train,
                   full_x_test,
                   repartition_count)
    
    full_mode = False
    spark_active.stop()

def merge_asm_bytes_json(train_bytes_file, test_bytes_file, train_asm_file, \
        test_asm_file, output_directory, output_train_filename, \
        output_test_filename):
    """
    Writes the ASM and bytes training and testing data into JSON files
    :return:
    :param train_bytes_file: Link or URL to train_bytes JSON file generated by build_all_JSON fucntions
    :param test_bytes_file: Link or URL to test_bytes JSON file generated by build_all_JSON functions
    :param train_asm_file: Link or URL to train_asm JSON file generated by build_all_JSON functions
    :param test_asm_file: Link or URL to test_asm JSON file generated by build_all_JSON functions
    :param output_directory: Link or URL to the output directory
    :param output_train_filename: Filename containing merged training data of both JSONs
    :param output_test_filename: Filename containing merged testing data of both JSONs
    :return: Returns JSON file containing bytes and ASM data
    """
    spark_active = SparkSession.builder.master("yarn").appName(
                    "DSP P1- Train & Test set creation").getOrCreate()
    
    #Reading Train Bytes and Asm files
    train_bytes = spark_active.read.json(train_bytes_file)
    test_bytes = spark_active.read.json(test_bytes_file)
    train_bytes = train_bytes.withColumnRenamed("bytes_text",train_bytes.text)
    test_bytes = train_bytes.withColumnRenamed("bytes_text",test_bytes.text)
    
    #Reading Test Bytes and Asm files
    train_asm = spark_active.read.json(train_asm_file)
    test_asm = spark_active.read.json(test_asm_file)
    train_bytes = train_bytes.withColumnRenamed("asm_text",train_bytes.text)
    test_bytes = train_bytes.withColumnRenamed("asm_text",test_bytes.text)
    
    #Joining Train and Test files respectively
    train_df = train_bytes.select("bytes_text","row_id").join(train_asm,"row_id",how = "left").select("row_id","bytes_text","asm_text","category")
    test_df = test_bytes.select("bytes_text","row_id").join(test_asm,"row_id",how = "left").select("row_id","bytes_text","asm_text")
    
    #Concatinating bytestext and asmtext columns into a single column
    train_set = train_df.withColumn('text',lower(concat(col("bytes_text"),lit(" "),col("asm_text"))))
    test_set = test_df.withColumn('text',lower(concat(col("bytes_text"),lit(" "),col("asm_text"))))
    
    
    train_set.write.json(output_directory + output_train_filename,mode = 'overwrite')
    test_set.write.json(output_directory + output_test_filename,mode = 'overwrite')

    spark_active.stop()

